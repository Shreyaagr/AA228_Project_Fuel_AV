"""This Code is adapted from: https://github.com/yanxi0830/CS238CrosswalkDriving/blob/main/src/simple/SimpleCrosswalkEnv.py"""import numpy as npimport gymfrom gym import spacesclass SimEnv(gym.Env):    metadata = {'render.modes': ['console']}    def __init__(self, config):        """        :param config: see config.py        """        super(SimEnv, self).__init__()        # action space        # ["no_change", "speed_up", "slow_down"]        self.actions_list   = config['actions_list']        self.action2control = {0: 0, 1: 1, 2: -1} # accelerations        self.action_space = spaces.Discrete(len(self.actions_list))        # state space: [position, velocity]        self.state_features = config['state_features']        self.init_state_pos = config['init_state_pos']         self.init_state_vel = config['init_state_vel']         self.goal_state_pos = config['goal_state_pos']          self.min_position = config['min_position']        self.max_position = config['max_position']        self.min_velocity = config['min_velocity']        self.max_velocity = config['max_velocity']        self.road_grade   = config['road_grade'] # LIST based on position index        self.observation_space = spaces.Box(low=np.array([0, 0]),                                            high=np.array([self.max_position, self.max_velocity]),                                            shape=(2,),                                            dtype=np.float32)        # fixed rewards_dict (i.e. independent of current state)        self.fixed_rewards_dict = config['fixed_rewards_dict']        # current state np.array([position, velocity])        self.state = np.array([self.init_state_pos, self.init_state_vel])        self.prev_state = self.state    def reset(self):        """        Return observation as np.array        :return: observation (np.array)        """        self.state = np.array([self.init_state_pos, self.init_state_vel])        return self.state    def _get_reward(self, action):        # return reward, done        reward = 0        done = False        # reward_goal        if self.state[0] >= (self.goal_state_pos):            reward += self.fixed_rewards_dict['reached_goal']            done = True        else:            # penalise for time-spent on road            reward += self.fixed_rewards_dict['per_step_cost']                        # penalise based on power/fuel consumption            cur_road_gr = self.road_grade[self.prev_state[0]]            reward += -cur_road_gr*self.prev_state[1] - 0.1*(self.prev_state[1]^2)            acc = self.action2control[action] # action : {0,1,2}            reward += -0.1*(acc)        return reward, done    def step(self, action):        # print(self.state)        # action: 0=no_change, 1=speed_up, 2=slow_down        #reward, done = self._get_reward(action)            # remember prev state        self.prev_state = self.state        # update velocity (cannot be negative)        self.state[1] += self.action2control[action]        self.state[1] = max(self.state[1], 0)        self.state[1] = min(self.state[1], self.max_velocity)        # update position        # new_pos = old_pos + velocity        self.state[0] += self.state[1]        self.state[0] = min(self.max_position, self.state[0])        reward, done = self._get_reward(action)        return self.state, reward, done, {}    def render(self, mode='console'):        if mode != 'console':            raise NotImplementedError()        # print(self.state)    def close(self):        pass